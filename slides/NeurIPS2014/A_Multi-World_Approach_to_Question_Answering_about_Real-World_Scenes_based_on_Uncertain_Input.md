#### A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input
###### Mateusz Malinowski, Mario Fritz
<div class="container">
  <div class="col">
    <u><b>Abstract</b></u><br>
    The DAtaset for QUestion Answering on Real-world images (or DAQUAR), released in 2015, was the first dataset and benchmark released for the VQA task.<br>
    <u><b>How to Make</b></u><br>
    It takes images from the NYU-Depth V2 dataset which contains images along with their semantic segmentations. We generated question answer pairs in two ways.
    <ol>
      <li>
        <b>Automatically, using question templates</b><br>
        We define 9 templates for questions, whose answers can be taken from the existing NYU-Depth V2 dataset annotations.
      </li>
      <li>
        <b>Using human annotations</b><br>
        We asked 5 participants to generate questions and answers with the constraint that answers must be either colors, numbers or classes or sets of these.
      </li>
    </ol><br>
    The resultant dataset contains a total of 12468 question-answer pairs.
    <u><b>Links</b></u><br>
    <ul>
      <li><a href="https://arxiv.org/abs/1410.0210">paper</a></li>
      <li><a href="https://www.cs.toronto.edu/~vendrov/Multi-WorldQ&A_Class_Presentation.pdf">slide</a></li>
    </ul><br>
  </div>
  <div class="col">
    <img width="100%" src="https://www.mpi-inf.mpg.de/fileadmin/_processed_/1/d/csm_qa_graphical_62dd34df8c.jpg">
  </div>
</div>