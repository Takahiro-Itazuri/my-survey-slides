#### Viewpoint-aware Video Summarization
###### Atsushi Kanehira, Luc Van Gool, Yoshitaka Ushiku, Tatsuya Harada
<div class="container">
  <div class="col">
    <u><b>Abstract</b></u><br>
    This paper introduces a novel variant of video summarization, namely building a summary that depends on the particular aspect of a video the viewer focuses on. We refer to this as viewpoint. The semantic similarity between videos in a group vs. the dissimilarity between groups is used to produce viewpoint-specific summaries. For considering similarity as well as avoiding redundancy, output summary should be (A) diverse, (B) representative of videos in the same group, and (C) discriminative against videos in the different groups. Inspired by Fisher's discriminant criteria, it selects summary by optimizing the combination of three terms (a) inner-summary, (b) inner-group, (c) between-group variances defined on the feature representation of summary. Moreover, we developed a novel dataset to investigate how well the generated summary reflects the underlying viewpoint.<br>
    <u><b>Contribution</b></u><br>
    <ul>
      <li>Propose a novel video summarization method from multiple groups of videos where their similarity are taken into consideratio</li>
      <li>Develop a novel dataset for quantitative evaluation</li>
      <li>Demonstrate the effectiveness of proposed method by quantitative and qualitative experiments on the dataset</li>
    </ul><br>
    <u><b>Links</b></u><br>
    <ul>
      <li><a href="https://arxiv.org/abs/1804.02843">paper</a></li>
      <li><a href="http://akanehira.github.io/">author</a></li>
    </ul><br>
  </div>
  <div class="col">
    <img width="100%" src="">
    <img width="100%" src="">
    <img width="100%" src="">
  </div>
</div>