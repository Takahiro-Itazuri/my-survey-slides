#### Exploring Models and Data for Image Question Answering
###### Mangye Ren, Ryan Kiros, Richard Zemel
<div class="container">
  <div class="col">
    <u><b>Abstract</b></u><br>
    The COCO-QA dataset for VQA is dataset based on MS-COCO. 
    Both questions and answers are generated automarically using image captions from MS-COCO and broadly belong to four categories: Object, Number, Color and Location. There is one question per image and answers are single-world. 
    Evaluation is done using either accuracy or WUPS score. <br>
    <u><b>Contribution</b></u><br>
    <ul>
      <li>a generic end-to-end QA model using visual semantic embedding to connect a CNN and a reccurent neural net (RNN)</li>
      <li>an automatic question generation algorithm that converts description sentences into questions</li>
      <li>a new SQ dataset (COCO-QA) that was generated using the algorithm, and a number of baseline results on this new dataset</li>
    </ul><br>
    <u><b>Links</b></u><br>
    <ul>
      <li><a href="https://arxiv.org/abs/1505.02074">paper</a></li>
      <li><a href="https://github.com/renmengye/imageqa-public">GitHub</a></li>
    </ul><br>
  </div>
  <div class="col">
    <img width="100%" src="">
    <img width="100%" src="">
    <img width="100%" src="">
  </div>
</div>