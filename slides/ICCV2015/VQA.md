#### VQA: Visual Question Answering
###### Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh
<div class="container">
  <div class="col">
    <u><b>Abstract</b></u><br>
    We propose the task of free-form and open-ended Visual Question Answering (VQA). 
    It is divided into two parts: one dataset contains real-world images from MS-COCO, and another dataset contains abstract clipart scenes created from models of humans and animals to remove the need to process noisy images and only perform high level reasoning. 
    Questions and answers are generated from crowd-sourced workers and 10 answers are obtained for each question from unique workers. 
    For evaluation, both open-ended answer generation as well as multiple choice formats are available.<br>
    <u><b>Links</b></u><br>
    <ul>
      <li><a href="https://arxiv.org/abs/1505.00468">paper</a></li>
      <li><a href="http://www.visualqa.org/">project</a></li>
      <li><a href="https://youtu.be/gar88i-V0RE">demo</a></li>
    </ul><br>
  </div>
  <div class="col">
    <img width="100%" src="http://www.visualqa.org/static/img/teaser_small.jpg">
    <img width="100%" src="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs11263-016-0966-6/MediaObjects/11263_2016_966_Fig8_HTML.gif">
  </div>
</div>