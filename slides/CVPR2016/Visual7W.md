#### Visual7W: Grounded Question Answering in Images
###### Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei
<div class="container">
  <div class="col">
    <u><b>Abstract</b></u><br>
    Visual7W is a dataset generated using images from the MS-COCO dataset for image captioning, recognition and segmentation. 
    The Visual7W dataset gets it name from generating multiple-choice questions of the form (Who, What, Where, When, Why, How and Which).
    Workers on Amazon Mechanical Turk (AMT) were used to generate the questions. 
    A separate set of three workers were used to rate the questions and those with less than two positive votes were discarded. 
    Multiple choice answers were generated both automatically and by AMT workers.
    AMT workers were also asked to draw bounding boxes of object mentioned in the question in the image, firstly resolve textual anbiguity, and secondly to enable answers of a visual nature. 
    The dataset contains 47,300 images and 327,939 questions.<br>
    <u><b>Links</b></u><br>
    <ul>
      <li><a href="multi-world approach to question answering about real-world scenes based on uncertain input">paper</a>
      <li><a href="https://github.com/yukezhu/visual7w-toolkit">GitHub</a></li>
      <li><a href="https://www.slideshare.net/xavigiro/visual7w-grounded-question-answering-in-images">slide</a></li>
    </ul><br>
  </div>
  <div class="col">
    <img width="100%" src="https://www.groundai.com/media/arxiv_projects/8892/pull_figure.svg">
    <img width="100%" src="http://web.stanford.edu/~yukez/images/img/visual7w_examples.png">
  </div>
</div>